# -*- coding: utf-8 -*-
"""04_product_analyse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiZzAFIgC5kHr44oSDP5AolOC9_t0i2Q
"""

!pip install dataframe-image

# Commented out IPython magic to ensure Python compatibility.
!git clone -q https://github.com/xn-projects/it-school-analytics.git
# %cd it-school-analytics

from utils import (
    setup_logging,
    load_files,
    log_section,
    get_my_palette,
    prod_analysis
)

import logging
import os
import datetime
import time
import dataframe_image as dfi

import numpy as np
import pandas as pd

setup_logging()
logging.info('Repository successfully loaded and ready')

PROJECT_ROOT = '/content/it-school-analytics'
CLEAN_DIR = os.path.join(PROJECT_ROOT, 'data', 'clean')

log_section('=== Downloading source Excel files ===')

BASE_URL = 'https://raw.githubusercontent.com/xn-projects/it-school-analytics/main/data/clean/'
FILES = ['calls_clean.xlsx', 'contacts_clean.xlsx', 'deals_clean.xlsx', 'spend_clean.xlsx']

download = load_files(BASE_URL, FILES)
logging.info(f'Downloaded {len(download)} files successfully.')

log_section('=== Reading Excel files ===')

df_calls = pd.read_excel(os.path.join(CLEAN_DIR, 'calls_clean.xlsx'),
                         dtype={'Id': str, 'CONTACTID':str}, engine='openpyxl')
df_contacts = pd.read_excel(os.path.join(CLEAN_DIR, 'contacts_clean.xlsx'),
                         dtype={'Id': str}, engine='openpyxl')
df_deals = pd.read_excel(os.path.join(CLEAN_DIR, 'deals_clean.xlsx'),
                         dtype={'Id': str, 'Contact Name':str}, engine='openpyxl')
df_spend = pd.read_excel(os.path.join(CLEAN_DIR, 'spend_clean.xlsx'),
                         engine='openpyxl')

logging.info('All files successfully loaded into DataFrames.')

log_section('=== Calculate unit-economics ===')
start_time = time.time()

"""# 1. Calculate unit economics by product and identify the growth point.

### **Unit Economics Metrics**
<font size="3">

**UA** — User Acquisition: scaling units<br>
**С1** - Conversion to First Purchase: conversion rate<br>
**AOV** - Average Order Value: average check or transaction value<br>
**CPA** — Cost per Acquisition: cost of acquiring one scaling unit (user)<br>
**APC** - Average Payment Count: purchase frequency (number of purchases per customer)<br>
</font>
"""

prod_analysis(df_deals, df_contacts, df_spend)

prod_analysis(df_deals, df_contacts, df_spend, product='Digital Marketing')

prod_analysis(df_deals, df_contacts, df_spend, product='Web Developer')

prod_analysis(df_deals, df_contacts, df_spend, product='UX/UI Design')

"""# 2. Metric Tree
*(Based on the core datasets used in business performance analysis)*  

<br>

## 1️⃣ Level — Target Metric  

- **CM — Contribution Margin**  

  $CM = UA \times (LTV - CPA)$

---

## 2️⃣ Level — Unit Economics Metrics  

- **UA — User Acquisition**

  $df\_contacts[Id].nunique()$

- **C1 — Conversion to First Purchase**

  $C1 = B / UA$

- **AOV — Average Order Value**

  $AOV = REV / T$

- **CPA — Cost per Acquisition**

  $CPA = AC / UA$

- **APC — Average Payment Count**

  $APC = T / B$

---

## 3️⃣ Level — Product Metrics

- **AOVᵢ — Average Order Value (per user)**  

  $AOV_i = \frac{Initial\ Amount\ Paid + \frac{(Offer\ Total\ Amount - Initial\ Amount\ Paid)}{Course\ duration - 1} \times (Months\ of\ study - 1)}{Months\ of\ study}$

- **Rᵢ — Revenue (per user)**  

  $R_i = AOV_i \times T$

- **REV — Total Revenue**  

  $REV = [R_i].sum()$ (for Total)<br>
  $REV = AOV \times T$

- **B — Buyers**  

  $B = [Stage] == Payment\ Done$ (for Total)  
  $B = C1 \times UA$

- **LTV — Lifetime Value**  

  $LTV = CLTV \times C1$

- **T — Transactions**  

  $T = [Months\ of\ study].sum()$ (for Total)  
  $T = B \times APC$

- **CLTV — Customer Lifetime Value**  
  
  $CLTV = AOV \times APC$

- **AC — Acquisition Cost**  
   
  $AC = df\_spend[Spend].sum()$ (for Total)  
  $AC = UA \times CPA$

---

## 4️⃣ Level — Atomic Metrics  
*(Base columns from each dataset used in the calculations)*  

**1. df_deals**  
- `Initial Amount Paid`  
- `Offer Total Amount`  
- `Course duration`  
- `Months of study`  
- `Stage (Payment Done)`  

**2. df_spend**  
- `Spend`  

**3. df_contacts**  
- `Id`  
<br>

*In this way, all metrics are hierarchically connected:<br>
**atomic → product → unit economics → target (CM)***

# 3. Identify which product metric they will influence and formulate hypotheses.

**Hypothesis 1:** <br>
For the Digital Marketing product, re-engaging users who have already shown interest but haven’t completed the target action can increase their readiness to convert.
Launching a campaign to reach this “warmed” audience through personalized messages and relevant ad formats (social media, email, push, SMS) is expected **to raise the conversion rate by 2% — from 2.52% to 2.57%.**
Such an increase would confirm the effectiveness of repeated interactions and show that this approach can be a reliable tool for improving conversion in the Digital Marketing product line.

**Hypothesis 2:** <br>
For the Web Developer product, offering new users a one-month free trial will increase engagement during the trial period and lead to a higher conversion rate to paid plans after the trial ends.<br>
It is expected that providing users with the opportunity to test the product without financial commitment will **boost conversion by 4% — from 0.73% to 0.76%** over the full observation period.

Such an increase would confirm that a free trial effectively improves user activation and payment conversion for the Web Developer product.

**Hypothesis 3:**<br>
For the UX/UI Design product, displaying authentic customer reviews on the website, in the mobile app, and in email campaigns will increase audience trust and the likelihood of completing the target action.
It is assumed that showcasing real user experiences will **raise the conversion rate by 1% — from 1.21% to 1.22%.**

Such a result would confirm that customer reviews help build trust and serve as an effective tool for improving conversion.

# 4. Describe the hypothesis testing method and define the testing conditions

### Hypothesis 1

**H₀ (Null Hypothesis):**<br>
Re-engagement of users who have already shown interest does not lead to an increase in the conversion rate.<br>
The conversion rate of the re-engaged group (Group B) is equal to or lower than that of the control group (Group A).

**A/B Test:**<br>
Group A — users without re-engagement<br>
Group B — users exposed to re-engagement campaigns

**What is being tested:**<br>
Communication formats and channels (social media, email, push, SMS), creative content, and timing of delivery.

**Metrics:**<br>
 - Baseline conversion (p): 2.52%<br>
 - Target conversion: 2.65%<br>
 - Relative increase (x): +5%<br>
 - Total number of people: 18509<br>
 - Total number of days: 357<br>
 - Average number of people per day: 52

**Sample size:**<br>
157 users per group

**Success criterion:**<br>
Conversion rate increase of at least 5% — from 2.52% to 2.65%

**Formula**<br>
$n = \frac{16 \times p \times (1 - p)}{x^2}$

$day = \frac{n \times 2}{Average Daily Users​}$


**Conclusion:**<br>
The A/B test can be completed in approximately **6 days**, which means it can be comfortably executed within a two-week testing window.<br>
This timeframe allows not only for the test itself but also for data validation, monitoring of interim results, and post-test analysis, ensuring that the conclusions are both statistically reliable and operationally actionable.
"""

n = (16 * 0.0252 * (1 - 0.0252)) / (0.05 ** 2)
n

day = (157.21574399999997 * 2) / 52
day

"""### Hypothesis 2

**H₀ (Null Hypothesis):**<br>
Offering a one-month free trial does not increase the conversion rate after the trial.

**A/B Test:**<br>
Group A — users without a free trial<br>
Group B — users offered a one-month free trial

**What is being tested:**<br>
The presence and duration of the free trial offer.

**Metrics:**<br>
 - Baseline conversion (p): 0.73%<br>
 - Target conversion: 0.76%<br>
 - Relative increase (x): +4%<br>
 - Total number of people: 18509<br>
 - Total number of days: 357<br>
 - Average number of people per day: 52

**Sample size:**<br>
72 users per group

**Success criterion:**<br>
Conversion rate increase of at least 4% — from 0.73% to 0.76%

**Formula**<br>
$n = \frac{16 \times p \times (1 - p)}{x^2}$

$day = \frac{n \times 2}{Average Daily Users​}$


**Conclusion:**<br>
The A/B test can technically be conducted within **3 days** to collect the required sample size.<br>
However, because the experiment involves a **one-month free trial**, the actual results on conversion can only be measured **after the 30-day period ends**.
Therefore, the **full testing cycle** — including data collection and evaluation — will take approximately **one month**.
"""

n = (16 * 0.0073 * (1 - 0.0073)) / (0.04 ** 2)
n

day = (72.4671 * 2) / 52
day

"""### Hypothesis 3

**H₀ (Null Hypothesis):**<br>
Displaying customer reviews does not significantly affect the conversion rate compared to users who do not see reviews.<br>

**A/B Test:**<br>
Group A — users who do not see customer reviews<br>
Group B — users who see authentic customer reviews

**What is being tested:**<br>
The presence, format, and placement of customer reviews (website, app, email).

**Metrics:**<br>
 - Baseline conversion (p): 1.21%<br>
 - Target conversion: 1.23%<br>
 - Relative increase (x): +2%<br>
 - Total number of people: 18509<br>
 - Total number of days: 357<br>
 - Average number of people per day: 52

**Sample size:**<br>
501 users per group

**Success criterion:**<br>
Conversion rate increases by at least 2% - from 1.21% to 1.23%.

**Formula**<br>
$n = \frac{16 \times p \times (1 - p)}{x^2}$

$day = \frac{n \times 2}{Average Daily Users​}$


**Conclusion:**<br>
The required sample size for this A/B test can be reached in approximately **19 days** under current traffic conditions.
Since the experiment does not require a delayed observation period, results can be analyzed immediately after the data collection phase.<br>
However, given this duration, the test **cannot be fully completed within the planned two-week timeframe**, and an extension of the testing period will be required to obtain statistically valid results.
"""

n = (16 * 0.0127 * (1 - 0.0127)) / (0.02 ** 2)
n

day = (501.54839999999996 * 2) / 52
day

logging.info(f'All hypotheses have been successfully tested in {time.time() - start_time:.2f} sec.')