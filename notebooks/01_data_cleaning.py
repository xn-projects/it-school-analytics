# -*- coding: utf-8 -*-
"""01_data_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yS15bJhcQ6PASZV2wtYgufiqxWI6J2tv
"""

pip install dataframe-image

# Commented out IPython magic to ensure Python compatibility.
!git clone -q https://github.com/xn-projects/it-school-analytics.git
# %cd it-school-analytics

from utils import (
    setup_logging,
    show_df,
    log_section,
    DataSummary,
    find_duplicates,
    clean_duplicates,
    convert_columns,
    frequent_non_null,
    clean_amount,
    normalize_german_level,
    convert_to_seconds,
    convert_to_minutes,
    convert_to_hours,
    load_files,
    save_table_as_png,
    save_plot,
    save_clean_data,
)

import logging
import os
import datetime
import re
import requests
import warnings
import time

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

import dataframe_image as dfi

warnings.filterwarnings('ignore')

setup_logging()
logging.info('Repository successfully loaded and ready')

log_section('=== Downloading source Excel files ===')

base_url = 'https://raw.githubusercontent.com/xn-projects/it-school-analytics/main/data/raw/'
files = ['calls.xlsx', 'contacts.xlsx', 'deals.xlsx', 'spend.xlsx']

download = load_files(base_url, files)
logging.info(f'Downloaded {len(download)} files successfully.')

log_section('=== Reading Excel files ===')

df_calls = pd.read_excel(f'/content/it-school-analytics/data/raw/calls.xlsx', dtype={'Id': str, 'CONTACTID':str}, engine='openpyxl')
df_contacts = pd.read_excel(f'/content/it-school-analytics/data/raw/contacts.xlsx', dtype={'Id': str}, engine='openpyxl')
df_deals = pd.read_excel(f'/content/it-school-analytics/data/raw/deals.xlsx', dtype={'Id': str, 'Contact Name':str}, engine='openpyxl')
df_spend = pd.read_excel(f'/content/it-school-analytics/data/raw/spend.xlsx', engine='openpyxl')

logging.info('All files successfully loaded into DataFrames.')

"""# === Calls ==="""

log_section('=== Analyzing Calls dataset ===')
start_time = time.time()

calls_info = DataSummary('calls_info_raw', df_calls)
summary_calls_info = calls_info.summary_info()

display(summary_calls_info)

"""#### Remove irrelevant columns"""

df_calls.drop(['Dialled Number', 'Tag'], axis=1, inplace=True)
logging.info('Dropped irrelevant columns: Dialled Number, Tag.')

"""#### Search and remove dublicates"""

duplicates = find_duplicates(df_calls, 'calls_raw')

clean_calls = clean_duplicates(df_calls, 'calls_clean')

"""#### Convert columns to datetime"""

clean_calls = convert_columns(clean_calls, datetime_cols=['Call Start Time'])

"""#### Analyze calls with missing CONTACTID"""

calls_nan_contactid = clean_calls[clean_calls['CONTACTID'].isna()]
clean_calls["CONTACTID"] = clean_calls["CONTACTID"].astype(str)

logging.info(f'Missing CONTACTID rows: {calls_nan_contactid.shape[0]}')
show_df(calls_nan_contactid.head(5))

"""#### Fill missing Call duration with 0 and add flag"""

clean_calls.loc[
    clean_calls['Call Duration (in seconds)'].isna(),
    'Call Type'
].unique()

clean_calls.loc[
    clean_calls['Call Duration (in seconds)'].isna(),
    'Outgoing Call Status'
].unique()

clean_calls['Is_duration_missing'] = (
    clean_calls['Call Duration (in seconds)']
    .isna()
)

clean_calls['Call Duration (in seconds)'] = (
    clean_calls['Call Duration (in seconds)']
    .fillna(0)
    .astype('Int64')
)
logging.info('Filled missing Call Duration values with 0 and added Is_duration_missing flag.')

"""#### Checking missing values in 'Outgoing Call Status'"""

calls_nan_status = clean_calls[clean_calls['Outgoing Call Status'].isna()]
logging.info(f'Missing Outgoing Call Status: {calls_nan_status.shape[0]} rows')

clean_calls['Outgoing Call Status'] = (
    clean_calls['Outgoing Call Status']
    .fillna('Unknown')
)
logging.info('Filled missing Outgoing Call Status with Unknown.')

"""#### Fill missing 'Scheduled in CRM' with 0 and add flag"""

clean_calls['Is_schedule_missing'] = (
    clean_calls['Scheduled in CRM']
    .isna()
)

clean_calls['Scheduled in CRM'] = (
    clean_calls['Scheduled in CRM']
    .fillna(0)
    .astype('float')
)
logging.info('Filled missing Scheduled in CRM with 0.0 and added boolean flag Is_schedule_missing.')

"""#### Analytical check: identifying abnormal call durations"""

mean_duration = clean_calls['Call Duration (in seconds)'].mean()
std_duration = clean_calls['Call Duration (in seconds)'].std()
upper_limit = mean_duration + 3 * std_duration

Q1 = clean_calls['Call Duration (in seconds)'].quantile(0.25)
Q3 = clean_calls['Call Duration (in seconds)'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5 * IQR
upper_limit_iqr = Q3 + 1.5 * IQR

outliers_iqr = clean_calls[
    (clean_calls['Call Duration (in seconds)'] < lower_limit) |
    (clean_calls['Call Duration (in seconds)'] > upper_limit_iqr)
]

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(
    clean_calls['Call Duration (in seconds)'],
    bins=50,
    color='cornflowerblue',
    alpha=0.7,
    label='Core calls'
)
axes[0].hist(
    outliers_iqr['Call Duration (in seconds)'],
    bins=50,
    color='tomato',
    alpha=0.7,
    label='Outliers (IQR)'
)

axes[0].axvline(lower_limit, color='tomato', linestyle='--', linewidth=2, label='Lower IQR limit')
axes[0].axvline(upper_limit_iqr, color='tomato', linestyle='--', linewidth=2, label='Upper IQR limit')
axes[0].axvline(mean_duration, color='seagreen', linestyle='--', linewidth=2, label=f'Mean = {mean_duration:.1f}')

axes[0].set_title('Distribution of Call Durations — IQR Outliers')
axes[0].set_xlabel('Duration (seconds)')
axes[0].set_ylabel('Frequency')
axes[0].legend()

outliers_std = clean_calls[
    clean_calls['Call Duration (in seconds)'] > upper_limit
]

axes[1].scatter(
    clean_calls['Call Start Time'],
    clean_calls['Call Duration (in seconds)'],
    alpha=0.7,
    color='cornflowerblue',
    label='Core calls'
)
axes[1].scatter(
    outliers_std['Call Start Time'],
    outliers_std['Call Duration (in seconds)'],
    alpha=0.7,
    color='tomato',
    label='Outliers (> 3σ)'
)
axes[1].axhline(upper_limit, color='tomato', linestyle='--', linewidth=2, label='3σ limit')
axes[1].axhline(mean_duration, color='seagreen', linestyle='--', linewidth=2, label=f'Mean = {mean_duration:.1f}')

axes[1].set_title('Call Durations Over Time with Outliers')
axes[1].set_xlabel('Call Start Time')
axes[1].set_ylabel('Duration (seconds)')
axes[1].legend()

fig.suptitle('Outlier Detection: IQR vs 3σ Rule', fontsize=14, fontweight='bold')
plt.tight_layout()

save_plot('outlier_detection_iqr_vs_3sigma', subfolder='notebooks', fig=fig)
plt.show()

logging.info(
    f'Outlier stats — mean: {mean_duration:.2f}, std: {std_duration:.2f}, '
    f'outliers: {outliers_std.shape[0]}'
)
logging.info('\n' + str(outliers_std['Call Duration (in seconds)'].describe().round(1)))

"""####Convert columns to category types"""

categorical_cols = [
    'Call Type',
    'Call Status',
    'Outgoing Call Status'
]

clean_calls = convert_columns(clean_calls, category_cols=categorical_cols)

info_calls = DataSummary('calls_info_clean', clean_calls)
summary_info_calls = info_calls.summary_info()

display(summary_info_calls)
logging.info(f'Calls cleaning completed in {time.time() - start_time:.2f} sec.')

"""# === Contacts ==="""

log_section('=== Analyzing Contacts dataset ===')
start_time = time.time()

contacts_info = DataSummary('contacts_info_raw', df_contacts)
summary_contacts_info = contacts_info.summary_info()

display(summary_contacts_info)

"""#### Search and remove dublicates"""

duplicates = find_duplicates(df_contacts, 'contacts_raw')

clean_contacts = clean_duplicates(df_contacts, 'contacts_clean')

bool_rows = clean_contacts[clean_contacts['Contact Owner Name'].isin([True, False])]

logging.info(f'Rows with True/False in Contact Owner Name: {bool_rows.shape[0]}')
show_df(bool_rows.head(5))

clean_contacts = clean_contacts[clean_contacts['Contact Owner Name'] != False]
logging.info(f'Removed invalid True/False Contact Owner Name values.')

"""#### Convert columns to datetime"""

datetime_cols = ['Created Time', 'Modified Time']
clean_contacts = convert_columns(clean_contacts, datetime_cols=datetime_cols)

"""#### Find rows where Created Time is later than Modified Time"""

condition = clean_contacts['Created Time'] > clean_contacts['Modified Time']

if condition.any():
    logging.info(f'Rows where Created Time > Modified Time: {condition.sum()}')
else:
    logging.info('No rows where Created Time > Modified Time.')

info_contacts = DataSummary('contacts_info_clean', clean_contacts)
summary_info_contacts = info_contacts.summary_info()

display(summary_info_contacts)
logging.info(f'Contacts cleaning completed in {time.time() - start_time:.2f} sec.')

"""# === Spend ==="""

log_section('=== Analyzing Spend dataset ===')
start_time = time.time()

spend_info = DataSummary('spend_info_raw', df_spend)
summary_spend_info = spend_info.summary_info()

display(summary_spend_info)

"""#### Search and remove duplicates"""

duplicates = find_duplicates(df_spend, 'spend_raw', ignore_first_col=False)

clean_spend = clean_duplicates(df_spend, 'spend_clean', ignore_first_col=False)

"""#### Search and remove test rows"""

test_rows = clean_spend[clean_spend['Source'] == 'Test']
logging.info(f'Found {test_rows.shape[0]} test rows.')

clean_spend = clean_spend.drop(test_rows.index)
logging.info(f'Rows after removing test data: {clean_spend.shape[0]}')

"""#### Replace missing fields with Unknown"""

cols = ['Campaign', 'AdGroup', 'Ad']

for col in cols:
    if col in clean_spend.columns:
        clean_spend[col] = clean_spend[col].fillna('Unknown')
logging.info(f'Filled missing values in {cols} with Unknown.')

"""####  Convert to category types"""

clean_spend = convert_columns(clean_spend, category_cols=['Source'])

info_spend = DataSummary('spend_info_clean', clean_spend)
summary_info_spend = info_spend.summary_info()

display(summary_info_spend)
logging.info(f'Spend cleaning completed in {time.time() - start_time:.2f} sec.')

"""# === Deals ==="""

log_section('=== Analyzing Deals dataset ===')
start_time = time.time()

deals_info = DataSummary('deals_info_raw', df_deals)
summary_deals_info = deals_info.summary_info()

display(summary_deals_info)

"""#### Detecting duplicate contact names and filling missing Cities, Level of Deutsch and Deal Owner Name"""

cols_to_fill = ['City', 'Level of Deutsch', 'Deal Owner Name']

for col in cols_to_fill:
    before_missing = df_deals[col].isna().sum()

    df_deals[col] = (
        df_deals
        .groupby('Contact Name')[col]
        .transform(frequent_non_null)
        .combine_first(df_deals[col])
    )

    after_missing = df_deals[col].isna().sum()
    filled = before_missing - after_missing
    logging.info(f'Filled {filled} missing values in {col}')

col = 'Course duration'
before_missing = df_deals[col].isna().sum()

df_deals[col] = (
    df_deals
    .groupby(['Contact Name', 'Product'])['Course duration']
    .transform(frequent_non_null)
    .combine_first(df_deals[col])
)

after_missing = df_deals[col].isna().sum()
filled = before_missing - after_missing
logging.info(f'Filled {filled} missing values in {col}')

"""#### Search and remove duplicates"""

duplicates = find_duplicates(df_deals, 'deals_raw')

clean_deals = clean_duplicates(df_deals, 'deals_clean')

duplicates = clean_deals[clean_deals['Lost Reason'] == 'Duplicate']
logging.info(f'Rows with Lost Reason = Duplicate: {duplicates.shape[0]}')

clean_deals = clean_deals[clean_deals['Lost Reason'] != 'Duplicate']
logging.info(f'Rows after removing Duplicate deals: {clean_deals.shape[0]}')

"""#### Identify and remove rows marked as test data"""

test_rows = clean_deals[clean_deals['Source'] == 'Test']
logging.info(f'Test rows found: {test_rows.shape[0]}')

clean_deals = clean_deals.drop(test_rows.index)
logging.info(f'Rows after removing test rows: {clean_deals.shape[0]}')

test_rows = clean_deals[
    clean_deals['Page']
    .isin(['/test', '/eng/test'])
]
logging.info(f'Found {test_rows.shape[0]} test rows in deals dataset.')

clean_deals = clean_deals.drop(test_rows.index)
logging.info(f'Rows after removing test rows: {clean_deals.shape[0]}')

missing_id_rows = clean_deals[clean_deals['Id'].isna()]

logging.info(f'Found {len(missing_id_rows)} rows with missing Id.')
show_df(missing_id_rows, name='Rows with missing Id')

clean_deals = clean_deals.dropna(subset=['Id'])
logging.info(f'Rows after removing rows with missing Id: {clean_deals.shape[0]}')

"""#### Find and remove products Data Analytics, Find Yourself in IT"""

target_products = ['Data Analytics', 'Find yourself in IT']
rows_to_remove = clean_deals[clean_deals['Product'].isin(target_products)]

logging.info(f'Rows with target products: {rows_to_remove.shape[0]}')

clean_deals = clean_deals.drop(rows_to_remove.index)
logging.info(f'Removed rows with products: {target_products}. Remaining rows: {clean_deals.shape[0]}')

"""#### Find duplicates in Contact Name"""

duplicates = clean_deals[clean_deals['Contact Name'].duplicated(keep=False)]

duplicate_names_count = clean_deals['Contact Name'].duplicated(keep=False).sum()
unique_duplicate_names = clean_deals.loc[
    clean_deals['Contact Name'].duplicated(keep=False), 'Contact Name'
].nunique()

logging.info(f'Found {len(duplicates)} duplicate rows for {unique_duplicate_names} Contact Names.')
show_df(duplicates.sort_values(by='Contact Name'), name='Duplicate Contact Names')

important_cols = [
    'Course duration',
    'Months of study',
    'Initial Amount Paid',
    'Offer Total Amount',
    'Education Type',
    'Product',
    'Payment Type',
    'Campaign'
]

empty_cols = clean_deals[important_cols].isna().all(axis=1)

duplicates_cols = clean_deals['Contact Name'].duplicated(keep=False)

before = len(clean_deals)

clean_deals = clean_deals[~(duplicates_cols & empty_cols)].copy()

after = len(clean_deals)
removed = before - after

logging.info(f'Removed {removed} duplicate rows with empty key fields.')
show_df(clean_deals, name='Clean Deals (duplicates with empty fields removed)')

"""#### Convert columns to datetime"""

clean_deals = convert_columns(
    clean_deals,
    datetime_cols=['Created Time', 'Closing Date']
)

"""#### Manual correction of specific city values"""

clean_deals['City'] = (
    clean_deals['City']
    .astype(str)
    .str.replace(r'[\u2010\u2011\u2012\u2013\u2014\u2212]', '-', regex=True)
)

city_correction = {
    'Karl-Liebknecht str. 24, Hildburghausen, Thüringen': 'Thüringen',
    'Vor Ebersbach 1, 77761 Schiltach': 'Schiltach',
    'Poland , Gdansk , Al. Grunwaldzka 7, ap. 1a': 'Gdańsk'
}
clean_deals['City'] = clean_deals['City'].replace(city_correction)

logging.info(f'Normalized City names and applied {len(city_correction)} manual corrections.')

"""#### Cleaning and correcting amount values in deals"""

cols_to_clean = ['Initial Amount Paid', 'Offer Total Amount']

for col in cols_to_clean:
    before_clean = clean_deals[col].copy()

    clean_deals[col] = clean_deals[col].apply(clean_amount)

    changed = (before_clean != clean_deals[col]).sum()
    logging.info(f'Cleaned and converted {col} — {changed} values changed.')

check_time = clean_deals['Offer Total Amount'] < clean_deals['Initial Amount Paid']
temp_columns = clean_deals.loc[check_time, 'Offer Total Amount']

clean_deals.loc[check_time, 'Offer Total Amount'] = clean_deals.loc[check_time, 'Initial Amount Paid']
clean_deals.loc[check_time, 'Initial Amount Paid'] = temp_columns

logging.info(f'Swapped {check_time.sum()} rows where Offer Total Amount < Initial Amount Paid.')

"""#### Fill missing payment type"""

fill = clean_deals['Payment Type'].isna()

no_pay = (
    ((clean_deals['Offer Total Amount'] == 0) & (clean_deals['Initial Amount Paid'] == 0)) |
    (clean_deals['Initial Amount Paid'] == 0)
)

one_payment = (
    (clean_deals['Initial Amount Paid'] > 0) &
    (abs(clean_deals['Offer Total Amount'] - clean_deals['Initial Amount Paid']) <= 400)
)

reservation = (
    (clean_deals['Initial Amount Paid'] > 0) &
    (clean_deals['Initial Amount Paid'] <= 200)
)

recurring = ~(no_pay | one_payment | reservation)

clean_deals.loc[fill & no_pay, 'Payment Type'] = 'No Payments'
clean_deals.loc[fill & one_payment, 'Payment Type'] = 'One Payment'
clean_deals.loc[fill & reservation, 'Payment Type'] = 'Reservation'
clean_deals.loc[fill & recurring, 'Payment Type'] = 'Recurring Payments'

clean_deals.loc[fill & clean_deals['Payment Type'].isna(), 'Payment Type'] = 'Unknown'

payment_counts = (
    clean_deals.loc[fill, 'Payment Type']
    .value_counts(dropna=False)
    .rename_axis('Payment Type')
    .reset_index(name='Count')
)

logging.info('Payment Type counts after filling:\n%s', payment_counts)
logging.info(f'Filled {fill.sum()} missing Payment Type values.')
show_df(payment_counts.reset_index().rename(columns={'index': 'Payment Type', 'Payment Type': 'Count'}))

"""#### Find rows where Created Time is later than Closing Date"""

condition = clean_deals['Created Time'] > clean_deals['Closing Date']
logging.info(f'Rows where Created Time > Closing Date: {condition.sum()}')

temp = clean_deals.loc[condition, 'Created Time'].copy()
clean_deals.loc[condition, 'Created Time'] = clean_deals.loc[condition, 'Closing Date']
clean_deals.loc[condition, 'Closing Date'] = temp

logging.info(f'Swapped {condition.sum()} rows where Created Time was later than Closing Date.')

"""#### Fill Closing Date"""

clean_deals['Product'] = clean_deals['Product'].fillna('Unknown')

clean_deals['Days_Diff'] = (clean_deals['Closing Date'] - clean_deals['Created Time']).dt.days

mode_diff = clean_deals.groupby('Product')['Days_Diff'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

fill = (
    clean_deals['Closing Date'].isna() &
    (clean_deals['Months of study'] == clean_deals['Course duration']) &
    (clean_deals['Stage'] == 'Payment Done')
)

clean_deals.loc[fill, 'Closing Date'] = (
    clean_deals.loc[fill, 'Created Time'] +
    clean_deals.loc[fill, 'Product'].map(mode_diff).apply(lambda x: pd.Timedelta(days=x) if pd.notna(x) else pd.NaT)
)

logging.info(f'Filled {fill.sum()} missing Closing Date values.')

clean_deals.drop(columns=['Days_Diff'], inplace=True)
logging.info('Temporary column Days_Diff removed after filling.')

"""#### Normalize Level of Deutsch"""

clean_deals['German Level'] = clean_deals['Level of Deutsch'].apply(normalize_german_level)
logging.info('Created new column German Level with normalized values.')

level_counts = clean_deals['German Level'].value_counts(dropna=False)
logging.info(f'German Level distribution:\n{level_counts}')

clean_deals.drop(columns=['Level of Deutsch'], inplace=True)
logging.info('Removed original column Level of Deutsch.')

"""#### Identify and remove an outlier date in Created Time (October 2022)"""

earliest_dates = (
    clean_deals['Created Time']
    .dropna()
    .sort_values()
    .unique()[:5]
)

logging.info(f'Earliest 5 unique Created Time values:\n{earliest_dates}')
show_df(pd.DataFrame(earliest_dates, columns=['Earliest Created Time']))

earliest_date = earliest_dates[0]

earliest_rows = clean_deals[clean_deals['Created Time'] == earliest_date]
clean_deals = clean_deals[clean_deals['Created Time'] != earliest_date]

logging.info(f'Removed {earliest_rows.shape[0]} rows with Created Time = {earliest_date}')
show_df(earliest_rows, name='Removed Earliest Created Time Rows')

"""#### Column Quality transformation"""

clean_deals.loc[
    clean_deals['Quality'].str.fullmatch(r'F', case=False, na=False),
    'Quality'
] = 'Special'

clean_deals['Quality'] = clean_deals['Quality'].str.replace('–', '-', regex=False)
clean_deals['Quality'] = clean_deals['Quality'].str.extract(r'-\s*(.*)')[0].str.strip()
clean_deals['Quality'] = clean_deals['Quality'].fillna('Unknown')

logging.info(f'Unique values after transformation: {clean_deals["Quality"].unique()}')

"""#### Standardize column SLA"""

clean_deals['SLA'] = clean_deals['SLA'].apply(
    lambda x: x.strftime("%H:%M:%S") if isinstance(x, datetime.time) else x
)
clean_deals['SLA'] = pd.to_timedelta(clean_deals['SLA'], errors='coerce')

show_df(clean_deals['SLA'].value_counts(dropna=False))

clean_deals['SLA Seconds'] = clean_deals['SLA'].apply(convert_to_seconds)

total = len(clean_deals)
missing = clean_deals['SLA Seconds'].isna().sum()

logging.info(f'Failed to convert: {missing} ({missing / total:.2%})')
logging.info(f'Converted successfully: {total - missing} rows ({(total - missing) / total:.2%})')
logging.info(f'Min SLA: {clean_deals["SLA Seconds"].min()}, Max SLA: {clean_deals["SLA Seconds"].max()}')

check_sla = clean_deals[['SLA', 'SLA Seconds']]
show_df(check_sla, name='SLA vs SLA Seconds', max_rows=len(check_sla))

clean_deals['SLA Minutes'] = clean_deals['SLA'].apply(convert_to_minutes)

total = len(clean_deals)
missing = clean_deals['SLA Minutes'].isna().sum()

logging.info(f'Failed to convert: {missing} ({missing / total:.2%})')
logging.info(f'Converted successfully: {total - missing} rows ({(total - missing) / total:.2%})')
logging.info(f'Min SLA: {clean_deals["SLA Minutes"].min()}, Max SLA: {clean_deals["SLA Minutes"].max()}')

clean_deals['SLA Hours'] = clean_deals['SLA'].apply(convert_to_hours)

total = len(clean_deals)
missing = clean_deals['SLA Hours'].isna().sum()

logging.info(f'Failed to convert: {missing} ({missing / total:.2%})')
logging.info(f'Converted successfully: {total - missing} rows ({(total - missing) / total:.2%})')
logging.info(f'Min SLA: {clean_deals["SLA Hours"].min()}, Max SLA: {clean_deals["SLA Hours"].max()}')

clean_deals.drop(columns=['SLA'], inplace=True)

"""#### Fill empty as Unknown"""

cols = [
    'Lost Reason',
    'Education Type',
    'Deal Owner Name',
    'Content',
    'Term',
    'Campaign',
    'Contact Name'
]

clean_deals[cols] = clean_deals[cols].fillna('Unknown')

logging.info(f'Filled missing values with Unknown.')

"""#### Convert columns to int and category types"""

cols_int = [
    'Course duration',
    'Months of study',
    'Initial Amount Paid',
    'Offer Total Amount'
]

for col in cols_int:
    clean_deals[col] = clean_deals[col].astype('Int64')
    filled = clean_deals[col].notna().sum()
    logging.info(f'Converted {filled} non-null values.')

categ_cols = [
    'Stage',
    'Quality',
    'Source',
    'Product',
    'Lost Reason',
    'Education Type',
    'German Level',
    'Payment Type'
]

clean_deals = convert_columns(clean_deals, category_cols=categ_cols)

info_deals = DataSummary('deals_info_clean', clean_deals)
summary_info_deals = info_deals.summary_info()

display(summary_info_deals)

log_section('=== Saving results ===')
start_time = time.time()

save_clean_data(clean_calls, 'calls_clean')
save_clean_data(clean_contacts, 'contacts_clean')
save_clean_data(clean_deals, 'deals_clean')
save_clean_data(clean_spend, 'spend_clean')

save_table_as_png(summary_calls_info, 'calls_info_raw', subfolder='notebooks')
save_table_as_png(summary_info_calls, 'calls_info_clean', subfolder='notebooks')

save_table_as_png(summary_contacts_info, 'contacts_info_raw', subfolder='notebooks')
save_table_as_png(summary_info_contacts, 'contacts_info_clean', subfolder='notebooks')

save_table_as_png(summary_spend_info, 'spend_info_raw', subfolder='notebooks')
save_table_as_png(summary_info_spend, 'spend_info_clean', subfolder='notebooks')

save_table_as_png(summary_deals_info, 'deals_info_raw', subfolder='notebooks')
save_table_as_png(summary_info_deals, 'deals_info_clean', subfolder='notebooks')

save_table_as_png(bool_rows, name='contacts_bool_rows')
save_table_as_png(earliest_rows, name='deals_earliest_rows')
save_table_as_png(missing_id_rows, name='deals_missing_id')

logging.info(f'All cleaned datasets and visualizations successfully saved in {time.time() - start_time:.2f} sec.')
